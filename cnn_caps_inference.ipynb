{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To open this notebook in Google Colab, follow this [tutorial](https://www.endtoend.ai/blog/githubtocolab/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWu6EJVIgtVf"
      },
      "source": [
        "# CNN-CAPS Inference\n",
        "\n",
        "This notebook uses a trained cnn-caps model to perform inference on test images of CAPS ELS energy spectrograms.\n",
        "\n",
        "To run this notebook, upload test images into a `test` folder and modify the `filename` parameter in 'Predict single capsplot' section. Then run the cell for prediction. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOWdPfylg3jS",
        "outputId": "47911dc4-9c8b-4ef1-e58e-81f757e8b795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting grad-cam\n",
            "  Downloading grad-cam-1.4.3.tar.gz (7.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.8 MB 4.7 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from grad-cam) (3.2.2)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from grad-cam) (0.13.1+cu113)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from grad-cam) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from grad-cam) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from grad-cam) (4.64.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from grad-cam) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from grad-cam) (7.1.2)\n",
            "Requirement already satisfied: torch>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from grad-cam) (1.12.1+cu113)\n",
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.1->grad-cam) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.2->grad-cam) (2.23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->grad-cam) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->grad-cam) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->grad-cam) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->grad-cam) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->grad-cam) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.8.2->grad-cam) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->grad-cam) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->grad-cam) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->grad-cam) (1.1.0)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.4.3-py3-none-any.whl size=32263 sha256=9d04441bf532c8368aa9576f0911cfab871ca7bb61f92d942b3dc519976ead9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/05/47/36e06c7cdf46685b5a9e30686a1f93bff3e95a91bf1404c75d\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: ttach, grad-cam\n",
            "Successfully installed grad-cam-1.4.3 ttach-0.0.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ttach in /usr/local/lib/python3.7/dist-packages (0.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install grad-cam\n",
        "!pip install ttach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eWqB917hDKA"
      },
      "source": [
        "#Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PWEj4B58hAfe"
      },
      "outputs": [],
      "source": [
        "# general libraries\n",
        "import time\n",
        "import gc\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "%matplotlib inline\n",
        "import scipy\n",
        "from tqdm import trange\n",
        "from glob import glob\n",
        "import h5py\n",
        "from IPython import display\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "\n",
        "# ML metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, precision_recall_curve, average_precision_score, roc_curve, auc, classification_report\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# pytorch deep learning\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn, optim\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCC2NTEjhKhn"
      },
      "source": [
        "# Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "esspH0zRhH_b"
      },
      "outputs": [],
      "source": [
        "def load_model(path, model_filename=\"best.pt\"): \n",
        "    \"\"\" Load a pretrained model. \"\"\"\n",
        "    with open(os.path.join(path, model_filename), 'rb') as f:\n",
        "        if torch.cuda.is_available():\n",
        "            model = torch.load(f) \n",
        "        else:\n",
        "            model = torch.load(f, map_location=torch.device('cpu'))\n",
        "    return model.eval().to(device)\n",
        "\n",
        "\n",
        "def save_pkl(result, filename):\n",
        "    with open(filename, \"wb\") as f:\n",
        "        pickle.dump(result, f)\n",
        "\n",
        "\n",
        "def load_pkl(filename):\n",
        "    \"\"\" Load pickle file. \"\"\"\n",
        "    with open(filename, \"rb\") as f:\n",
        "        result = pickle.load(f)\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_img(filename):\n",
        "    \"\"\" Load .jpg or .png image as tensor. \"\"\"\n",
        "    img = tf.keras.preprocessing.image.load_img(filename, color_mode='rgb', target_size=(224,224))\n",
        "    img_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    img_arr = np.array([img_arr])\n",
        "    img_arr /= 255\n",
        "    img_tensor = torch.FloatTensor(img_arr).permute([0,3,1,2])\n",
        "    return img_tensor\n",
        "\n",
        "def plot_img_tensor(img_tensor, label=None):\n",
        "    \"\"\" Assume shape of img_tensor is (1,3,224,224) \"\"\"\n",
        "    plt.imshow(img_tensor.squeeze().permute(1,2,0))\n",
        "    if label is not None:\n",
        "        print('label: ', label.numpy())\n",
        "    plt.show()\n",
        "\n",
        "def preprocess_capsplot(filename, transform=None, plot=False):\n",
        "    \"\"\" Preprocess image for single capsplot prediction with model. \"\"\"\n",
        "    # load image\n",
        "    img = load_img(filename)\n",
        "\n",
        "    # normalise the image\n",
        "    resize_norm_transform = transforms.Compose([\n",
        "                                    transforms.Resize(224),\n",
        "                                    transforms.Normalize(mean=[0.403,0.647,0.577], std=[0.301,0.183,0.312]),\n",
        "                                  ])\n",
        "    img_stdscaled = resize_norm_transform(img)\n",
        "\n",
        "    if transform is not None:\n",
        "        img_stdscaled = transform(img_stdscaled)\n",
        "    \n",
        "    if plot:\n",
        "        plot_img_tensor(img)\n",
        "\n",
        "    return img_stdscaled\n",
        "\n",
        "def predict(filename, model, transform=None, plot=False):\n",
        "    \"\"\" Predict single capsplot with cnn-caps model. \"\"\"\n",
        "    img = preprocess_capsplot(filename, transform=transform, plot=plot)\n",
        "    result = model(torch.FloatTensor(img).to(device))[1].data.cpu().numpy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ph7J2H_uhZo5"
      },
      "source": [
        "## CONSTANTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rkiruNAchMFr"
      },
      "outputs": [],
      "source": [
        "# CONSTANTS\n",
        "N_CLASSES = 3\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.00016 # 0.00025 for softmax cross entropy, 0.00016 for EDL training\n",
        "RANDOM_SEED = 42\n",
        "#CLASS_LABELS = ['0_notCrossing', '1_MP', '2_BS']\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK4av0wx8Iaa"
      },
      "source": [
        "# ResNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KgLdY0vS8dPD"
      },
      "outputs": [],
      "source": [
        "# create layer that returns unchanged input\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zC9ejUNj8e3G"
      },
      "outputs": [],
      "source": [
        "class TransferredNet(nn.Module):\n",
        "    def __init__(self, pretrained_model):\n",
        "        super().__init__()\n",
        "        self.pretrained_model = pretrained_model\n",
        "        self.pretrained_model.fc = Identity()\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(512, 16),\n",
        "            #nn.ReLU(),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(16, N_CLASSES),\n",
        "            #nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "    def forward(self, input):\n",
        "        logits = self.head(self.pretrained_model(input))\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        return logits, probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "46464215bac9456d8a9b8bd351fa1c5c",
            "3256e19703e642bfba2e0cb2b897993d",
            "d2013532722b4b81ad839cac420c8f63",
            "06757d7b51994b5f89b8c5de537a37d9",
            "f5e6dbc7be7740398bff04a623978b47",
            "7d8c9d02dc9744c6a41af1d1bb5a7a80",
            "d8f80297dd0a468ea0848533025aea3f",
            "d0e412466a044986883a2aa4496b7a28",
            "1e80cf74a972428b97201d3972aecaac",
            "b55011d617564dceb6c807e30636854d",
            "f5bb8a47c03e4aeba1ca8aafa0a66545"
          ]
        },
        "id": "Owhb9QwQHadn",
        "outputId": "40f3b4ed-01fe-4c24-f826-be33fbd43381"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "46464215bac9456d8a9b8bd351fa1c5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# load pretrained model and freeze some of the parameters in the model\n",
        "pretrained_model = resnet18(pretrained=True)\n",
        "\n",
        "# COMMENT OUT BELOW IF WANT TO TRAIN THE WHOLE NETWORK\n",
        "# for i, child in enumerate(pretrained_model.children()):\n",
        "    \n",
        "#     # There are 9 blocks in resnet18 (last fc layer replaced with Identity)\n",
        "#     #print(i, child)\n",
        "\n",
        "#     # Let's freeze the weights of the first 2/3 of model\n",
        "#     if i <= 6:\n",
        "#         for param in child.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "model = TransferredNet(pretrained_model).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlMa1LGbEfiY",
        "outputId": "dee7e13e-c505-4ee3-e110-61d901e347fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PRETRAINED MODEL:\n",
            "\n",
            "0 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "4 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "5 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "6 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "7 Sequential(\n",
            "  (0): BasicBlock(\n",
            "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (downsample): Sequential(\n",
            "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (1): BasicBlock(\n",
            "    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            ")\n",
            "HEAD:\n",
            "\n",
            "0 Linear(in_features=512, out_features=16, bias=True)\n",
            "2 Linear(in_features=16, out_features=3, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# Show the blocks which require gradients (i.e. will be finetunned in training)\n",
        "\n",
        "print('PRETRAINED MODEL:\\n')\n",
        "for i, child in enumerate(model.pretrained_model.children()):\n",
        "    for param in child.parameters():\n",
        "        if param.requires_grad:\n",
        "            print(i, child)\n",
        "            break\n",
        "\n",
        "print('HEAD:\\n')\n",
        "for i, child in enumerate(model.head.children()):\n",
        "    for param in child.parameters():\n",
        "        if param.requires_grad:\n",
        "            print(i, child)\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8vl1S-EGzdw"
      },
      "source": [
        "# Predict single capsplot:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irg7B-UoRbRZ"
      },
      "source": [
        "## Predictions with Cross Entropy Model (Multiclass)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8UrCXXWDbTqW"
      },
      "outputs": [],
      "source": [
        "# Upload pretrained model to current runtime\n",
        "MODEL_NAME = \"resnet18_custom_softmaxCrossEntropy_train_full_multiclass.pt\"\n",
        "model = load_model(path=\"./\", model_filename=MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ew4cEU55jFXo",
        "outputId": "423be43e-eb8f-4ca4-adfb-decda90a95d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction [[0.63766557 0.2857451  0.07658938]]\n",
            "CPU times: user 15.9 ms, sys: 0 ns, total: 15.9 ms\n",
            "Wall time: 16.4 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Upload images from datasets/images/ to current runtime\n",
        "filename='/content/2007-02-02T220000_2007-02-03T010000.png' # correct prediction\n",
        "filename='/content/2007-03-16T130300_2007-03-16T150300.png' # incorrect prediction (however the caps data do not look strikingly clear)\n",
        "\n",
        "# predict\n",
        "result = predict(filename, model, plot=False)\n",
        "print('Prediction', result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e8AADs8T4sM"
      },
      "source": [
        "For cases where the prediction is uncertain, e.g. \n",
        "filename='/content/test/2007-03-16T130300_2007-03-16T150300.png' # incorrect prediction (however the caps data do not look strikingly clear). The predictions were: Prediction [[0.63766736 0.2857435  0.07658914]]. We could set a threshold for if the max predicted probability is less than 0.75, then this is an uncertain class and we should assign them to the 'human-in-the-loop' pile. See 'Define Data and Establish Baseline' page in MLOps course.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ5ianjDUF1_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn-caps-inference.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06757d7b51994b5f89b8c5de537a37d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b55011d617564dceb6c807e30636854d",
            "placeholder": "​",
            "style": "IPY_MODEL_f5bb8a47c03e4aeba1ca8aafa0a66545",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 133MB/s]"
          }
        },
        "1e80cf74a972428b97201d3972aecaac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3256e19703e642bfba2e0cb2b897993d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d8c9d02dc9744c6a41af1d1bb5a7a80",
            "placeholder": "​",
            "style": "IPY_MODEL_d8f80297dd0a468ea0848533025aea3f",
            "value": "100%"
          }
        },
        "46464215bac9456d8a9b8bd351fa1c5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3256e19703e642bfba2e0cb2b897993d",
              "IPY_MODEL_d2013532722b4b81ad839cac420c8f63",
              "IPY_MODEL_06757d7b51994b5f89b8c5de537a37d9"
            ],
            "layout": "IPY_MODEL_f5e6dbc7be7740398bff04a623978b47"
          }
        },
        "7d8c9d02dc9744c6a41af1d1bb5a7a80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55011d617564dceb6c807e30636854d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0e412466a044986883a2aa4496b7a28": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2013532722b4b81ad839cac420c8f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0e412466a044986883a2aa4496b7a28",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1e80cf74a972428b97201d3972aecaac",
            "value": 46830571
          }
        },
        "d8f80297dd0a468ea0848533025aea3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5bb8a47c03e4aeba1ca8aafa0a66545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5e6dbc7be7740398bff04a623978b47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
